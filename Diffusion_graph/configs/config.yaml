# Configuration file for Graph Diffusion Polymer Design Pipeline

# Paths
paths:
  data_dir: "Data"
  polymer_file: "Data/Polymer/SMiPoly_polymers.gz"
  property_dir: "Data/Property"
  results_dir: "results"
  checkpoints_dir: "results/checkpoints/graph_backbone_best.pt"
  figures_dir: "results/figures"
  metrics_dir: "results/metrics"

# Data processing
data:
  random_seed: 42
  unlabeled_train_ratio: 0.95
  unlabeled_val_ratio: 0.05
  property_train_ratio: 0.8
  property_val_ratio: 0.1
  property_test_ratio: 0.1

# Graph representation (values set dynamically by step0)
graph:
  Nmax: null  # Maximum atoms, determined from data
  atom_vocab_size: null  # Set from data analysis
  edge_vocab_size: 6  # {NONE, SINGLE, DOUBLE, TRIPLE, AROMATIC, MASK}

# Graph Backbone model (GraphGPS-style with edge-aware attention)
backbone:
  hidden_size: 384
  num_layers: 8
  num_heads: 8
  ffn_hidden_size: 1536
  dropout: 0.1
  max_position_embeddings: 256

# Small proxy model for hyperparameter tuning
proxy_backbone:
  hidden_size: 512
  num_layers: 4
  num_heads: 4
  ffn_hidden_size: 1024
  dropout: 0.1

# Model size presets for scaling law experiments (graph-specific)
model_sizes:
  small:
    # Architecture (~14M params)
    hidden_size: 384
    num_layers: 6
    num_heads: 6
    ffn_hidden_size: 1536
    dropout: 0.1
    # Training
    max_steps: 300000
    warmup_steps: 1000
    batch_size: 128
    gradient_accumulation_steps: 4
    learning_rate: 3.0e-4
  medium:
    # Architecture (~55M params)
    hidden_size: 640
    num_layers: 10
    num_heads: 10
    ffn_hidden_size: 2560
    dropout: 0.1
    # Training
    max_steps: 300000
    warmup_steps: 2000
    batch_size: 128
    gradient_accumulation_steps: 4
    learning_rate: 3.0e-4
  large:
    # Architecture (~140M params)
    hidden_size: 896
    num_layers: 14
    num_heads: 8
    ffn_hidden_size: 3584
    dropout: 0.1
    # Training
    max_steps: 300000
    warmup_steps: 3000
    batch_size: 128
    gradient_accumulation_steps: 8
    learning_rate: 1.0e-4
  xl:
    # Architecture (~350M params)
    hidden_size: 1152
    num_layers: 18
    num_heads: 16
    ffn_hidden_size: 4608
    dropout: 0.1
    # Training
    max_steps: 300000
    warmup_steps: 4000
    batch_size: 64
    gradient_accumulation_steps: 16
    learning_rate: 1.0e-4

# Diffusion settings
diffusion:
  num_steps: 50
  beta_min: 0.05
  beta_max: 0.95
  lambda_node: 1.0  # Weight for node prediction loss
  lambda_edge: 0.5  # Weight for edge prediction loss

# Training backbone
training_backbone:
  batch_size: 256  # Reduced for graph memory
  learning_rate: 3e-4
  weight_decay: 0.01
  warmup_steps: 1000
  max_steps: 300000
  gradient_clip_norm: 1.0
  eval_every: 1000
  save_every: 100000
  num_epochs: 50

# Training property heads
training_property:
  batch_size: 128
  learning_rate: 1e-3
  weight_decay: 0.01
  num_epochs: 500
  patience: 30
  freeze_backbone: true
  finetune_last_layers: 6
  default_timestep: 1

# Optimization settings for A100 GPU
optimization:
  use_amp: true                    # Mixed precision (BF16)
  compile_model: false             # Disabled for graph models (compatibility)
  gradient_accumulation_steps: 1
  num_workers: 0
  pin_memory: false
  cudnn_benchmark: true
  prefetch_factor: 1
  cache_tokenization: false  # Pre-encode graphs (disabled to prevent OOM with 19M+ graphs)

# Property head architecture
property_head:
  hidden_sizes: [256, 1024, 128]
  dropout: 0.1

# Hyperparameter tuning for property head (Step 3) using Optuna
hyperparameter_tuning:
  enabled: true                    # Enable via config or --tune flag
  n_trials: 50                      # Number of Optuna trials
  tuning_epochs: 50                 # Max epochs per trial
  tuning_patience: 10               # Early stopping patience per trial
  metric: "r2"                      # Optimization metric: maximize RÂ² on validation

  # Search space
  search_space:
    # Property head architecture: 3-5 layers, neurons from [64, 128, 256, 512, 1024]
    num_layers: [3, 4, 5]
    neurons: [64, 128, 256, 512, 1024]

    # Training hyperparameters
    learning_rate: [4.0e-4, 6.0e-4, 8.0e-4, 1.0e-3]
    dropout: [0.1, 0.2, 0.3]
    finetune_last_layers: [2, 4, 6]
    batch_size: [8, 16, 32, 64, 128]  # 2^3 to 2^7

# Sampling
sampling:
  num_samples: 10000
  batch_size: 256
  temperature: 1.0
  use_constraints: true

# Inverse design
inverse_design:
  num_candidates: 10000
  epsilon: 30.0

# Polymer classes (SMARTS patterns)
polymer_classes:
  polyimide: "[#6](=O)-[#7]-[#6](=O)"
  polyester: "[#6](=O)-[#8]-[#6]"
  polyamide: "[#6](=O)-[#7]-[#6]"
  polyurethane: "[#8]-[#6](=O)-[#7]"
  polyether: "[#6]-[#8]-[#6]"

# Plotting
plotting:
  figure_size: [4.5, 4.5]
  font_size: 12
  dpi: 600

# Legacy tokenizer config (for backward compatibility)
tokenizer:
  special_tokens:
    pad: "[PAD]"
    mask: "[MASK]"
    bos: "[BOS]"
    eos: "[EOS]"
    unk: "[UNK]"
  max_length: 128
